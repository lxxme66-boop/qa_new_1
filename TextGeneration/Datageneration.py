import argparse
import os
import re
import asyncio
import json
from datetime import datetime
# Optional imports with fallbacks
try:
    from volcenginesdkarkruntime import Ark, AsyncArk
    VOLC_AVAILABLE = True
except ImportError:
    VOLC_AVAILABLE = False
    print("Warning: volcenginesdkarkruntime not available")

try:
    from openai import OpenAI, AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("Warning: openai not available")

# Import local model support
try:
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from LocalModels.local_model_manager import LocalModelManager
    LOCAL_MODEL_SUPPORT = True
except ImportError:
    LOCAL_MODEL_SUPPORT = False
    print("Warning: Local model support not available")

from .prompts_conf import system_prompt, user_prompts

# Load configuration
config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config.json")
if os.path.exists(config_path):
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
        use_local_models = config.get('api', {}).get('use_local_models', False)
        local_model_config = config.get('models', {}).get('local_models', {}).get('ollama', {})
else:
    use_local_models = False
    local_model_config = {}

# API configuration
ark_url = "http://0.0.0.0:8080/v1"
api_key = "fake-key"
model = "/mnt/data/MLLM/liuchi/trained_models/Qwen3-32B-dpo-5w_retrain"

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
os.makedirs("TextGeneration/logs", exist_ok=True)
current_time = os.path.getmtime(__file__) if os.path.exists(__file__) else 0
file_handler = logging.FileHandler(f"TextGeneration/logs/text_generation_{current_time}.log")


# def extract_text_chunks(text_content, chunk_size=2000, overlap=200):
#     """
#     Extract text chunks from the content for processing.
#     """
#     chunks = []
#     text_length = len(text_content)
    
#     if text_length <= chunk_size:
#         return [text_content]
    
#     start = 0
#     while start < text_length:
#         end = min(start + chunk_size, text_length)
        
#         # Try to find a natural break point (sentence end)
#         if end < text_length:
#             for sep in ['\n\n', 'ã€‚\n', 'ã€‚', '.\n', '.', '\n']:
#                 last_sep = text_content[start:end].rfind(sep)
#                 if last_sep != -1:
#                     end = start + last_sep + len(sep)
#                     break
        
#         chunk = text_content[start:end]
#         chunks.append(chunk)
        
#         # Move start position with overlap
#         start = end - overlap if end < text_length else end
    
#     return chunks

def extract_text_chunks(text_content, chunk_size=2000, overlap=200):
    """
    é«˜æ•ˆæ–‡æœ¬åˆ†å—ï¼Œæ”¯æŒå¤§æ–‡ä»¶å¤„ç†
    """
    # å¤„ç†ç©ºæ–‡æœ¬
    if not text_content:
        return []
    
    # å¤„ç†å°æ–‡æœ¬
    text_length = len(text_content)
    if text_length <= chunk_size:
        return [text_content]
    
    chunks = []
    start = 0
    
    # æ·»åŠ è¿›åº¦è®¡æ•°å™¨
    total_chunks = (text_length - overlap) // (chunk_size - overlap) + 1
    processed_chunks = 0
    
    while start < text_length:
        # è®¡ç®—å½“å‰åˆ†å—ç»“æŸä½ç½®
        end = min(start + chunk_size, text_length)
        
        # æŸ¥æ‰¾æœ€ä½³åˆ†å¥ç‚¹ï¼ˆä¼˜å…ˆæ®µè½ï¼Œå†å¥å­ï¼‰
        break_positions = [
            text_content.rfind('\n\n', start, end),  # æ®µè½åˆ†éš”
            text_content.rfind('ã€‚', start, end),     # ä¸­æ–‡å¥å·
            text_content.rfind('.\n', start, end),    # è‹±æ–‡å¥å·+æ¢è¡Œ
            text_content.rfind('\n', start, end)      # æ™®é€šæ¢è¡Œ
        ]
        
        # é€‰æ‹©æœ€æ¥è¿‘æœ«å°¾çš„æœ‰æ•ˆåˆ†å¥ç‚¹
        valid_breaks = [pos for pos in break_positions if pos > start]
        if valid_breaks:
            end = max(valid_breaks) + 1  # åŒ…å«åˆ†å¥ç¬¦å·
        
        # æˆªå–åˆ†å—
        chunk = text_content[start:end]
        chunks.append(chunk)
        
        # æ›´æ–°è¿›åº¦
        processed_chunks += 1
        if processed_chunks % 10 == 0:  # æ¯10ä¸ªåˆ†å—è®°å½•ä¸€æ¬¡
            logger.info(f"å·²å¤„ç†åˆ†å— {processed_chunks}/{total_chunks} ({processed_chunks/total_chunks:.1%})")
        
        # ç§»åŠ¨èµ·å§‹ä½ç½®ï¼ˆç¡®ä¿ä¸å€’é€€ï¼‰
        next_start = end - overlap
        if next_start <= start:  # é˜²æ­¢æ­»å¾ªç¯
            next_start = end
        
        start = max(next_start, end - overlap)  # ç¡®ä¿å‘å‰ç§»åŠ¨
    
    logger.info(f"å…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬åˆ†å—")
#     return chunks
# async def parse_txt(file_path, index=9, config=None):
#     """
#     Parse txt file and create tasks for processing.
#     Now supports passing config for local model support.
#     """
#     try:
#         with open(file_path, 'r', encoding='utf-8') as f:
#             content = f.read()
#     except Exception as e:
#         logger.error(f"Error reading file {file_path}: {e}")
#         return []
    
#     if not content.strip():
#         logger.warning(f"Empty file: {file_path}")
#         return []
    
#     # Extract text chunks
#     chunks = extract_text_chunks(content)
#     logger.info(f"Extracted {len(chunks)} chunks from {file_path}")
    
#     tasks = []
#     for i, chunk in enumerate(chunks):
#         task = input_text_process(
#             chunk, 
#             os.path.basename(file_path),
#             chunk_index=i,
#             total_chunks=len(chunks),
#             prompt_index=index,
#             config=config
#         )
#         tasks.append(task)
    
#     return tasks
async def parse_txt(file_path: str, index: int, config: dict):
    """è§£ææ–‡æœ¬æ–‡ä»¶å¹¶ç”Ÿæˆå¤„ç†ä»»åŠ¡"""
    logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
    
    # è¯»å–æ–‡ä»¶å†…å®¹
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ–‡æœ¬åˆ†å—å¤„ç† (ä½¿ç”¨é…ç½®ä¸­çš„åˆ†å—å¤§å°)
    chunk_size = config.get("processing", {}).get("chunk_size", 1000)
    chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
    
    # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
    tasks = []
    for idx, chunk in enumerate(chunks):
        task = {
            "file_path": file_path,
            "chunk_index": idx,
            "content": chunk,
            "timestamp": datetime.now().isoformat()
        }
        tasks.append(task)
    
    logger.info(f"ä¸ºæ–‡ä»¶ {os.path.basename(file_path)} åˆ›å»º {len(tasks)} ä¸ªå¤„ç†ä»»åŠ¡")
    return tasks

# async def input_text_process(text_content, source_file, chunk_index=0, total_chunks=1, prompt_index=9, config=None):
#     """
#     Process text content using the specified prompt.
#     Supports both API and local model backends.
#     """
#     # Check if we should use local models
#     use_local = False
#     use_vllm_http = False
#     local_model_manager = None
    
#     if config and LOCAL_MODEL_SUPPORT:
#         use_local = config.get('api', {}).get('use_local_models', False)
#         use_vllm_http = config.get('api', {}).get('use_vllm_http', False)
        
#         if use_local or use_vllm_http:
#             try:
#                 # å¦‚æœä½¿ç”¨vLLM HTTPï¼Œç¡®ä¿é…ç½®æ­£ç¡®
#                 if use_vllm_http:
#                     local_config = config.get('models', {}).get('local_models', {})
#                     local_config['default_backend'] = 'vllm_http'
#                     config['models']['local_models'] = local_config
                
#                 local_model_manager = LocalModelManager(config)
#                 if not local_model_manager.is_available():
#                     logger.warning("Local models enabled but not available, falling back to API")
#                     use_local = False
#             except Exception as e:
#                 logger.error(f"Failed to initialize local model manager: {e}")
#                 use_local = False
    
#     try:
#         user_prompt = user_prompts[prompt_index]
        
#         # Format the prompt with the text content
#         # Check if the prompt expects markdown_content or text_content
#         if '{markdown_content}' in user_prompt:
#             formatted_prompt = user_prompt.format(
#                 markdown_content=text_content,
#                 source_file=source_file,
#                 chunk_info=f"(Chunk {chunk_index + 1}/{total_chunks})" if total_chunks > 1 else ""
#             )
#         elif '{text_content}' in user_prompt:
#             formatted_prompt = user_prompt.format(
#                 text_content=text_content,
#                 source_file=source_file,
#                 chunk_info=f"(Chunk {chunk_index + 1}/{total_chunks})" if total_chunks > 1 else ""
#             )
#         else:
#             formatted_prompt = user_prompt
        
#         # Generate response using appropriate backend
#         if use_local and local_model_manager:
#             # Use local model
#             logger.info(f"Using local model backend: {local_model_manager.get_backend_name()}")
#             content = await local_model_manager.generate(
#                 prompt=formatted_prompt,
#                 system_prompt=system_prompt,
#                 temperature=config.get('models', {}).get('qa_generator_model', {}).get('temperature', 0.8),
#                 max_tokens=config.get('models', {}).get('qa_generator_model', {}).get('max_tokens', 4096),
#                 top_p=config.get('models', {}).get('qa_generator_model', {}).get('top_p', 0.9)
#             )
#         else:
#             # Use API backend (original code)
#             if not config:
#                 # Use default API configuration
#                 ark_url = "http://0.0.0.0:8080/v1"
#                 api_key = "fake-key"
#                 model = "/mnt/data/MLLM/liuchi/trained_models/Qwen3-32B-dpo-5w_retrain"
#             else:
#                 # Use configuration from config
#                 api_config = config.get('api', {})
#                 ark_url = api_config.get('ark_url', "http://0.0.0.0:8080/v1")
#                 api_key = api_config.get('api_key', "fake-key")
#                 model = config.get('models', {}).get('default_model', "/mnt/data/MLLM/liuchi/trained_models/Qwen3-32B-dpo-5w_retrain")
            
#             client = AsyncOpenAI(
#                 api_key=api_key,
#                 base_url=ark_url
#             )
            
#             response = await client.chat.completions.create(
#                 model=model,
#                 messages=[
#                     {
#                         "role": "system",
#                         "content": system_prompt
#                     },
#                     {
#                         "role": "user", 
#                         "content": formatted_prompt
#                     }
#                 ],
#                 temperature=0.8,
#                 max_tokens=4096,
#                 top_p=0.9,
#             )
            
#             content = response.choices[0].message.content
        
#         # Structure the response
#         result = {
#             "content": content,
#             "source_file": source_file,
#             "chunk_index": chunk_index,
#             "total_chunks": total_chunks,
#             "text_content": text_content[:500] + "..." if len(text_content) > 500 else text_content
#         }
        
#         logger.info(f"Successfully processed chunk {chunk_index + 1}/{total_chunks} from {source_file}")
#         return result
        
#     except Exception as e:
#         logger.error(f"Error processing text from {source_file}: {e}")
#         return None

async def input_text_process(text_content, source_file, chunk_index=0, total_chunks=1, prompt_index=9, config=None):
    """
    Process text content using the specified prompt.
    Supports both API and local model backends.
    """
    # Check if we should use local models
    use_local = False
    use_vllm_http = False
    local_model_manager = None
    
    # ä»é…ç½®ä¸­è·å–å¿…è¦çš„APIè®¾ç½®
    api_base = "http://localhost:8000/v1"
    api_key = "EMPTY"  # é»˜è®¤å€¼
    model_name = "qwen-vllm"
    timeout_value = 120.0
    # æ·»åŠ åˆ†å—æ ‡è¯†
    logger.info(f"å¼€å§‹å¤„ç†åˆ†å— {chunk_index+1}/{total_chunks} - å¤§å°: {len(text_content)}å­—ç¬¦")
    
    # åœ¨APIè¯·æ±‚å‰æ·»åŠ æ—¥å¿—
    logger.info(f"å‡†å¤‡APIè¯·æ±‚: {formatted_prompt[:100]}...")
    
    # åœ¨è¯·æ±‚åæ·»åŠ æ—¥å¿—
    logger.info(f"æ”¶åˆ°å“åº”: {content[:100]}...")
    if config:
        api_config = config.get('api', {})
        # ä»é…ç½®è·å– API è®¾ç½®
        api_base = api_config.get('vllm_server_url', "http://localhost:8000/v1")
        api_key = api_config.get('api_key', "EMPTY")  # ä½¿ç”¨é…ç½®ä¸­çš„å€¼
        use_local = api_config.get('use_local_models', False)
        use_vllm_http = api_config.get('use_vllm_http', False)
        
        models_config = config.get('models', {})
        model_name = models_config.get('qa_generator_model', {}).get('name', "qwen-vllm")
        
        # è·å–è¶…æ—¶è®¾ç½®ï¼Œä¼˜å…ˆä» vllm_http é…ç½®ä¸­è·å–
        vllm_http_config = config.get('models', {}).get('local_models', {}).get('vllm_http', {})
        timeout_value = vllm_http_config.get('timeout', 120.0)
        
        if use_local or use_vllm_http:
            try:
                # å¦‚æœä½¿ç”¨vLLM HTTPï¼Œç¡®ä¿é…ç½®æ­£ç¡®
                if use_vllm_http:
                    local_config = config.get('models', {}).get('local_models', {})
                    local_config['default_backend'] = 'vllm_http'
                    config['models']['local_models'] = local_config
                
                local_model_manager = LocalModelManager(config)
                if not local_model_manager.is_available():
                    logger.warning("Local models enabled but not available, falling back to API")
                    use_local = False
            except Exception as e:
                logger.error(f"Failed to initialize local model manager: {e}")
                use_local = False
                local_model_manager = None
    
    try:
        user_prompt = user_prompts[prompt_index]
        
        # Format the prompt with the text content
        if '{markdown_content}' in user_prompt:
            formatted_prompt = user_prompt.format(
                markdown_content=text_content,
                source_file=source_file,
                chunk_info=f"(Chunk {chunk_index + 1}/{total_chunks})" if total_chunks > 1 else ""
            )
        elif '{text_content}' in user_prompt:
            formatted_prompt = user_prompt.format(
                text_content=text_content,
                source_file=source_file,
                chunk_info=f"(Chunk {chunk_index + 1}/{total_chunks})" if total_chunks > 1 else ""
            )
        else:
            formatted_prompt = user_prompt
        
        # è®°å½•ä½¿ç”¨çš„åç«¯
        backend = "Local Model" if use_local and local_model_manager else "API"
        logger.info(f"ğŸ“¡ Using backend: {backend} for {source_file} chunk {chunk_index + 1}/{total_chunks}")
        
        # Generate response using appropriate backend
        if use_local and local_model_manager:
            # Use local model
            logger.info(f"ğŸ”§ Using local model backend: {local_model_manager.get_backend_name()}")
            content = await local_model_manager.generate(
                prompt=formatted_prompt,
                system_prompt=system_prompt,
                temperature=config.get('models', {}).get('qa_generator_model', {}).get('temperature', 0.8),
                max_tokens=config.get('models', {}).get('qa_generator_model', {}).get('max_tokens', 4096),
                top_p=config.get('models', {}).get('qa_generator_model', {}).get('top_p', 0.9)
            )
        else:
            # ä½¿ç”¨ API åç«¯
            logger.info(f"ğŸ”Œ Connecting to: {api_base}")
            logger.info(f"ğŸ”‘ API key: {api_key[:3]}...")  # æ˜¾ç¤ºéƒ¨åˆ† key
            logger.info(f"ğŸ¤– Model: {model_name}")
            logger.info(f"â±ï¸ Timeout: {timeout_value} seconds")
            
            # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
            client = AsyncOpenAI(
                api_key=api_key,
                base_url=api_base,
                timeout=timeout_value  # å®¢æˆ·ç«¯çº§è¶…æ—¶
            )
            
            try:
                # å‘é€è¯·æ±‚
                response = await client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {
                            "role": "system",
                            "content": system_prompt
                        },
                        {
                            "role": "user", 
                            "content": formatted_prompt
                        }
                    ],
                    temperature=config.get('models', {}).get('qa_generator_model', {}).get('temperature', 0.8),
                    max_tokens=config.get('models', {}).get('qa_generator_model', {}).get('max_tokens', 4096),
                    top_p=config.get('models', {}).get('qa_generator_model', {}).get('top_p', 0.9),
                    timeout=timeout_value  # è¯·æ±‚çº§è¶…æ—¶
                )
                
                content = response.choices[0].message.content
                logger.info(f"âœ… API request successful, response length: {len(content)} characters")
                
            except Exception as api_error:
                # è¯¦ç»†é”™è¯¯å¤„ç†
                error_msg = str(api_error)
                logger.error(f"âŒ API request failed: {error_msg}")
                
                # æ ¹æ®é”™è¯¯ç±»å‹æä¾›å»ºè®®
                if "connect" in error_msg.lower():
                    logger.error("ğŸ’¡ Connection failed, please check service status: "
                                 "1. Ensure vLLM service is running\n"
                                 "2. Check network connection\n"
                                 "3. Verify API address")
                elif "timeout" in error_msg.lower():
                    logger.error(f"ğŸ’¡ Request timeout, try increasing timeout (current: {timeout_value} seconds)")
                elif "authentication" in error_msg.lower():
                    logger.error("ğŸ’¡ Authentication failed, please check API key")
                elif "model not found" in error_msg.lower():
                    logger.error("ğŸ’¡ Model not found, please check model name")
                else:
                    logger.error("ğŸ’¡ Unknown error, please check logs")
                
                # è¿”å›Noneè¡¨ç¤ºå¤±è´¥
                return None
        
        # Structure the response
        result = {
            "content": content,
            "source_file": source_file,
            "chunk_index": chunk_index,
            "total_chunks": total_chunks,
            "text_content": text_content[:500] + "..." if len(text_content) > 500 else text_content
        }
        
        logger.info(f"âœ… Successfully processed chunk {chunk_index + 1}/{total_chunks} from {source_file}")
        return result
        
    except Exception as e:
        # å¤„ç†æ‰€æœ‰å…¶ä»–å¼‚å¸¸
        logger.error(f"âŒ Unknown error processing text ({source_file}): {e}")
        logger.exception("Detailed error:")  # è®°å½•å †æ ˆè·Ÿè¸ª
        return None
def merge_chunk_responses(responses):
    """
    Merge responses from multiple chunks of the same file.
    """
    if not responses:
        return []
    
    # Group by source file
    file_groups = {}
    for resp in responses:
        if resp and 'source_file' in resp:
            source = resp['source_file']
            if source not in file_groups:
                file_groups[source] = []
            file_groups[source].append(resp)
    
    # Merge chunks for each file
    merged_responses = []
    for source_file, chunks in file_groups.items():
        # Sort by chunk index
        chunks.sort(key=lambda x: x.get('chunk_index', 0))
        
        # Combine content
        combined_content = []
        all_text_content = []
        
        for chunk in chunks:
            combined_content.append(chunk['content'])
            all_text_content.append(chunk.get('text_content', ''))
        
        merged_response = {
            "content": "\n\n".join(combined_content),
            "source_file": source_file,
            "full_text": "\n".join(all_text_content)
        }
        
        merged_responses.append(merged_response)
    
    return merged_responses


async def process_folder_async(folder_path, prompt_index=9, max_concurrent=5, config=None):
    """
    å¼‚æ­¥å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
    """
    tasks = []
    
    # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.txt'):
                file_path = os.path.join(root, file)
                # ä½¿ç”¨parse_txtå¤„ç†æ–‡ä»¶
                file_tasks = await parse_txt(file_path, prompt_index, config)
                tasks.extend(file_tasks)
    
    # é™åˆ¶å¹¶å‘æ•°é‡
    results = []
    for i in range(0, len(tasks), max_concurrent):
        batch = tasks[i:i + max_concurrent]
        batch_results = await asyncio.gather(*batch, return_exceptions=True)
        # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœ
        for result in batch_results:
            if not isinstance(result, Exception) and result is not None:
                results.append(result)
            elif isinstance(result, Exception):
                logger.error(f"Task failed with error: {result}")
    
    return results


async def process_folder_async_with_history(folder_path, history_file=None, prompt_index=9, max_concurrent=5, config=None):
    """
    å¼‚æ­¥å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ–‡æœ¬æ–‡ä»¶ï¼Œæ”¯æŒå†å²è®°å½•
    """
    processed_files = set()
    
    # è¯»å–å†å²è®°å½•
    if history_file and os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
                for item in history_data:
                    if 'source_file' in item:
                        processed_files.add(item['source_file'])
            logger.info(f"Loaded {len(processed_files)} processed files from history")
        except Exception as e:
            logger.error(f"Error loading history file: {e}")
    
    tasks = []
    
    # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.txt') and file not in processed_files:
                file_path = os.path.join(root, file)
                # ä½¿ç”¨parse_txtå¤„ç†æ–‡ä»¶
                file_tasks = await parse_txt(file_path, prompt_index, config)
                tasks.extend(file_tasks)
    
    logger.info(f"Found {len(tasks)} new tasks to process")
    
    # é™åˆ¶å¹¶å‘æ•°é‡
    results = []
    for i in range(0, len(tasks), max_concurrent):
        batch = tasks[i:i + max_concurrent]
        batch_results = await asyncio.gather(*batch, return_exceptions=True)
        # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœ
        for result in batch_results:
            if not isinstance(result, Exception) and result is not None:
                results.append(result)
            elif isinstance(result, Exception):
                logger.error(f"Task failed with error: {result}")
    
    return results