#!/usr/bin/env python3
"""
åŠå¯¼ä½“æ˜¾ç¤ºæŠ€æœ¯æ–‡æœ¬QAæ•°æ®é›†ç”Ÿæˆç³»ç»Ÿ - æ–‡æœ¬ä¸“ç”¨ç‰ˆæœ¬
ä¸“é—¨ç”¨äºå¤„ç†çº¯æ–‡æœ¬å†…å®¹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„é—®ç­”å¯¹æ•°æ®é›†
"""

import os
import sys
import json
import asyncio
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from TextGeneration.text_qa_processor import TextQAProcessor
from TextGeneration.Datageneration import parse_txt
from TextQA.quality_assessment import QualityAssessment

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('text_qa_generation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

async def run_text_qa_pipeline(
    config: dict,
    input_dir: str = "data/texts",
    output_dir: str = "data/text_qa_results",
    batch_size: int = 2,
    quality_threshold: float = 0.7
):
    """
    è¿è¡Œæ–‡æœ¬QAç”Ÿæˆæµæ°´çº¿
    
    Args:
        config: é…ç½®å­—å…¸
        input_dir: è¾“å…¥æ–‡æœ¬ç›®å½•
        output_dir: è¾“å‡ºç»“æœç›®å½•
        batch_size: æ‰¹å¤„ç†å¤§å°
        quality_threshold: è´¨é‡é˜ˆå€¼
    """
    logger.info("=" * 80)
    logger.info("ğŸš€ å¯åŠ¨åŠå¯¼ä½“æ˜¾ç¤ºæŠ€æœ¯æ–‡æœ¬QAæ•°æ®é›†ç”Ÿæˆç³»ç»Ÿ")
    logger.info("=" * 80)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    chunks_dir = os.path.join(output_dir, "chunks")
    qa_results_dir = os.path.join(output_dir, "qa_results")
    os.makedirs(chunks_dir, exist_ok=True)
    os.makedirs(qa_results_dir, exist_ok=True)
    
    text_files = []
    
    try:
        # ===== ç¬¬ä¸€é˜¶æ®µï¼šæ–‡æœ¬é¢„å¤„ç† + QAç”Ÿæˆ =====
        logger.info("ç¬¬ä¸€é˜¶æ®µ: æ–‡æœ¬é¢„å¤„ç†å’ŒQAç”Ÿæˆ")
        
        # æ­¥éª¤1.1: æ–‡æœ¬åˆ†å—å’Œé¢„å¤„ç†
        logger.info("æ­¥éª¤1.1: æ–‡æœ¬åˆ†å—å’Œé¢„å¤„ç†...")
        all_tasks = []
        
        for root, _, files in os.walk(input_dir):
            for file in files:
                if file.endswith('.txt'):
                    file_path = os.path.join(root, file)
                    text_files.append(file_path)
                    
                    # ä½¿ç”¨ç°æœ‰çš„parse_txtå‡½æ•°è¿›è¡Œæ–‡æœ¬åˆ†å—
                    file_tasks = await parse_txt(file_path, index=9, config=config)
                    
                    if file_tasks:
                        logger.info(f"ä¸ºæ–‡ä»¶ {file} åˆ›å»ºäº† {len(file_tasks)} ä¸ªå¤„ç†ä»»åŠ¡")
                        all_tasks.extend(file_tasks)
        
        if not all_tasks:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æœ¬æ–‡ä»¶")
            return []
        
        logger.info(f"æ€»å…±åˆ›å»ºäº† {len(all_tasks)} ä¸ªæ–‡æœ¬å¤„ç†ä»»åŠ¡")
        
        # æ­¥éª¤1.2: æ–‡æœ¬QAç”Ÿæˆå¤„ç† (æ”¹è¿›ç‰ˆæœ¬)
        logger.info("æ­¥éª¤1.2: æ–‡æœ¬QAç”Ÿæˆå¤„ç†...")
        
        # åˆå§‹åŒ–æ–‡æœ¬QAå¤„ç†å™¨
        text_qa_processor = TextQAProcessor(config)
        
        # æ‰¹é‡å¤„ç†æ–‡æœ¬ï¼Œç”Ÿæˆé—®ç­”å¯¹
        processed_results = await text_qa_processor.batch_process_texts(all_tasks, batch_size)
        
        # ä¿å­˜QAç”Ÿæˆç»“æœ
        qa_results_file = os.path.join(chunks_dir, "text_qa_generated.json")
        with open(qa_results_file, 'w', encoding='utf-8') as f:
            json.dump(processed_results, f, ensure_ascii=False, indent=2)
        
        total_qa_count = sum(result.get("total_qa_count", 0) for result in processed_results)
        logger.info(f"æ–‡æœ¬QAç”Ÿæˆå®Œæˆï¼Œæ€»å…±ç”Ÿæˆäº† {total_qa_count} ä¸ªé—®ç­”å¯¹")
        
        if not processed_results:
            logger.error("æ²¡æœ‰QAç”Ÿæˆç»“æœï¼Œæµç¨‹ç»ˆæ­¢")
            return []
        
        # ===== ç¬¬äºŒé˜¶æ®µï¼šQAè´¨é‡è¯„ä¼°å’Œè¿‡æ»¤ =====
        logger.info("ç¬¬äºŒé˜¶æ®µ: QAè´¨é‡è¯„ä¼°å’Œè¿‡æ»¤")
        
        # æ­¥éª¤2.1: æå–æ‰€æœ‰é—®ç­”å¯¹è¿›è¡Œè´¨é‡è¯„ä¼°
        logger.info("æ­¥éª¤2.1: æå–å’Œæ•´ç†é—®ç­”å¯¹...")
        
        all_qa_pairs = []
        qa_statistics = {
            "total_texts": len(processed_results),
            "total_qa_pairs": 0,
            "by_type": {"factual": 0, "comparative": 0, "reasoning": 0, "open_ended": 0},
            "by_difficulty": {"basic": 0, "intermediate": 0, "advanced": 0}
        }
        
        for result in processed_results:
            source_file = result.get("source_file", "unknown")
            chunk_index = result.get("chunk_index", 0)
            
            for q_type, qa_list in result.get("qa_pairs", {}).items():
                for qa in qa_list:
                    # æ·»åŠ å…ƒæ•°æ®
                    qa_with_meta = {
                        **qa,
                        "source_file": source_file,
                        "chunk_index": chunk_index,
                        "qa_id": f"{source_file}_{chunk_index}_{q_type}_{len(all_qa_pairs)}"
                    }
                    all_qa_pairs.append(qa_with_meta)
                    
                    # ç»Ÿè®¡ä¿¡æ¯
                    qa_statistics["total_qa_pairs"] += 1
                    qa_statistics["by_type"][q_type] = qa_statistics["by_type"].get(q_type, 0) + 1
                    difficulty = qa.get("difficulty", "intermediate")
                    qa_statistics["by_difficulty"][difficulty] = qa_statistics["by_difficulty"].get(difficulty, 0) + 1
        
        logger.info(f"æå–äº† {len(all_qa_pairs)} ä¸ªé—®ç­”å¯¹è¿›è¡Œè´¨é‡è¯„ä¼°")
        logger.info(f"é—®ç­”å¯¹ç±»å‹åˆ†å¸ƒ: {qa_statistics['by_type']}")
        logger.info(f"é—®ç­”å¯¹éš¾åº¦åˆ†å¸ƒ: {qa_statistics['by_difficulty']}")
        
        # æ­¥éª¤2.2: è´¨é‡è¯„ä¼°
        logger.info("æ­¥éª¤2.2: é—®ç­”å¯¹è´¨é‡è¯„ä¼°...")
        
        # åˆå§‹åŒ–è´¨é‡è¯„ä¼°å™¨
        quality_assessor = QualityAssessment(config)
        
        # æ‰¹é‡è´¨é‡è¯„ä¼°
        evaluated_qa_pairs = []
        for i in range(0, len(all_qa_pairs), batch_size):
            batch = all_qa_pairs[i:i+batch_size]
            logger.info(f"è¯„ä¼°æ‰¹æ¬¡ {i//batch_size + 1}/{(len(all_qa_pairs)-1)//batch_size + 1}")
            
            batch_results = await asyncio.gather(
                *(quality_assessor.assess_qa_quality(qa) for qa in batch),
                return_exceptions=True
            )
            
            for j, result in enumerate(batch_results):
                if not isinstance(result, Exception) and result is not None:
                    qa_with_quality = {**batch[j], "quality_assessment": result}
                    evaluated_qa_pairs.append(qa_with_quality)
                else:
                    logger.error(f"è´¨é‡è¯„ä¼°å¤±è´¥: {result}")
        
        # æ­¥éª¤2.3: æ ¹æ®è´¨é‡é˜ˆå€¼è¿‡æ»¤
        logger.info("æ­¥éª¤2.3: æ ¹æ®è´¨é‡é˜ˆå€¼è¿‡æ»¤é—®ç­”å¯¹...")
        
        high_quality_qa_pairs = []
        quality_stats = {"passed": 0, "failed": 0, "total": len(evaluated_qa_pairs)}
        
        for qa in evaluated_qa_pairs:
            quality_score = qa.get("quality_assessment", {}).get("overall_score", 0)
            if quality_score >= quality_threshold:
                high_quality_qa_pairs.append(qa)
                quality_stats["passed"] += 1
            else:
                quality_stats["failed"] += 1
        
        logger.info(f"è´¨é‡è¿‡æ»¤å®Œæˆ: {quality_stats}")
        logger.info(f"é«˜è´¨é‡é—®ç­”å¯¹æ•°é‡: {len(high_quality_qa_pairs)}")
        
        # ===== ç¬¬ä¸‰é˜¶æ®µï¼šç»“æœæ•´ç†å’Œä¿å­˜ =====
        logger.info("ç¬¬ä¸‰é˜¶æ®µ: ç»“æœæ•´ç†å’Œä¿å­˜")
        
        # ä¿å­˜è¯„ä¼°åçš„æ‰€æœ‰é—®ç­”å¯¹
        all_qa_file = os.path.join(qa_results_dir, "all_qa_with_quality.json")
        with open(all_qa_file, 'w', encoding='utf-8') as f:
            json.dump(evaluated_qa_pairs, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é«˜è´¨é‡é—®ç­”å¯¹
        final_qa_file = os.path.join(qa_results_dir, "high_quality_qa_dataset.json")
        with open(final_qa_file, 'w', encoding='utf-8') as f:
            json.dump(high_quality_qa_pairs, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        final_stats = {
            **qa_statistics,
            "quality_stats": quality_stats,
            "quality_threshold": quality_threshold,
            "final_dataset_size": len(high_quality_qa_pairs),
            "processing_summary": {
                "input_files": len(text_files),
                "text_chunks": len(all_tasks),
                "generated_qa_pairs": qa_statistics["total_qa_pairs"],
                "high_quality_qa_pairs": len(high_quality_qa_pairs),
                "quality_pass_rate": quality_stats["passed"] / quality_stats["total"] if quality_stats["total"] > 0 else 0
            }
        }
        
        stats_file = os.path.join(output_dir, "processing_statistics.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, ensure_ascii=False, indent=2)
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ æ–‡æœ¬QAæ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
        logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        logger.info(f"   è¾“å…¥æ–‡ä»¶: {len(text_files)}")
        logger.info(f"   æ–‡æœ¬å—: {len(all_tasks)}")
        logger.info(f"   ç”Ÿæˆé—®ç­”å¯¹: {qa_statistics['total_qa_pairs']}")
        logger.info(f"   é«˜è´¨é‡é—®ç­”å¯¹: {len(high_quality_qa_pairs)}")
        logger.info(f"   è´¨é‡é€šè¿‡ç‡: {final_stats['processing_summary']['quality_pass_rate']:.2%}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        logger.info("=" * 80)
        
        return high_quality_qa_pairs
        
    except Exception as e:
        logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []

def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        return config
    except Exception as e:
        logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤é…ç½®
        return {
            "api": {
                "use_vllm_http": True,
                "vllm_server_url": "http://localhost:8000/v1",
                "api_key": "EMPTY"
            },
            "models": {
                "qa_generator_model": {
                    "name": "qwen-vllm",
                    "temperature": 0.7,
                    "max_tokens": 2048,
                    "timeout": 120.0
                }
            }
        }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŠå¯¼ä½“æ˜¾ç¤ºæŠ€æœ¯æ–‡æœ¬QAæ•°æ®é›†ç”Ÿæˆç³»ç»Ÿ')
    parser.add_argument('--input-dir', type=str, default='data/texts', help='è¾“å…¥æ–‡æœ¬ç›®å½•')
    parser.add_argument('--output-dir', type=str, default='data/text_qa_results', help='è¾“å‡ºç»“æœç›®å½•')
    parser.add_argument('--config', type=str, default='config_vllm_http.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch-size', type=int, default=2, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--quality-threshold', type=float, default=0.7, help='è´¨é‡é˜ˆå€¼(0-1)')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è¿è¡Œæµæ°´çº¿
    try:
        results = asyncio.run(run_text_qa_pipeline(
            config=config,
            input_dir=args.input_dir,
            output_dir=args.output_dir,
            batch_size=args.batch_size,
            quality_threshold=args.quality_threshold
        ))
        
        if results:
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(results)} ä¸ªé«˜è´¨é‡é—®ç­”å¯¹")
        else:
            logger.error("âŒ æœªèƒ½ç”Ÿæˆä»»ä½•é—®ç­”å¯¹")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()